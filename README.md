# Direct Preference Optimization (DPO)

This repository contains an implementation and analysis of **[Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290)**, a high-performance and lightweight technique for aligning large language models with human preferences.

This project explores the DPO algorithm as a modern alternative to traditional Reinforcement Learning from Human Feedback (RLHF).

## ðŸ“„ Project Report

For a complete breakdown of the methodology, findings, and analysis, please see the full project report.

[View the PDF Report](results/Direct_Preference_Optimization_Report.pdf)

## Repository Structure

* **/src**: Contains all source code for the DPO implementation, training, and evaluation.
* **/results**: Includes the final PDF report, model outputs, and experiment data.
* **/zipfiles**: Contains any compressed data or model files used in the project.
